import re
import logging
import hashlib
import requests
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from database import NewsDatabase
from services.telegram_client import TelegramClient
from utils.html_formatter import HTMLFormatter
from aiogram import Bot, Dispatcher, F
import asyncio

logger = logging.getLogger(__name__)

class CompanyReportsParser:
    def __init__(self, dp: Dispatcher, db: NewsDatabase):
        self.tg = TelegramClient(dp, db)
        self.db = db
        self.timeout = 30  # seconds
        
        # –°–ª–æ–≤–∞—Ä—å –∫–æ–º–ø–∞–Ω–∏–π: {–¥–æ–º–µ–Ω: (–Ω–∞–∑–≤–∞–Ω–∏–µ, —Ç–∏–∫–µ—Ä)}
        self.companies = {
            'inarctica.com': ('–ò–Ω–∞—Ä–∫—Ç–∏–∫–∞', 'AQUA'),
            'mmk.ru': ('–ú–ú–ö', 'MAGN'),  # –î–æ–±–∞–≤–ª—è–µ–º –ú–ú–ö —Å —Ç–∏–∫–µ—Ä–æ–º MAGN
            'vk.company': ('VK', 'VKCO'),  # –î–æ–±–∞–≤–ª—è–µ–º VK —Å —Ç–∏–∫–µ—Ä–æ–º VKCO
            'sollers-auto.com': ('–°–û–õ–õ–ï–†–°', 'SVAV'),  # –î–æ–±–∞–≤–ª—è–µ–º –°–û–õ–õ–ï–†–° —Å —Ç–∏–∫–µ—Ä–æ–º SVAV
            'ptsecurity.com': ('Positive Technologies', 'POSI'),  # –î–æ–±–∞–≤–ª—è–µ–º Positive Technologies
            'seligdar.ru': ('–°–µ–ª–∏–≥–¥–∞—Ä', 'SELG'),  # –î–æ–±–∞–≤–ª—è–µ–º –°–µ–ª–∏–≥–¥–∞—Ä
            'ozonpharm.ru': ('–û–∑–æ–Ω –§–∞—Ä–º–∞—Ü–µ–≤—Ç–∏–∫–∞', 'OZON'),
            # –î–æ–±–∞–≤—å—Ç–µ –¥—Ä—É–≥–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏
            # 'company2.com': ('–ù–∞–∑–≤–∞–Ω–∏–µ2', 'TICKER2'),
        }
    
    async def parse_ozonpharm(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç—á–µ—Ç–æ–≤ –û–∑–æ–Ω –§–∞—Ä–º–∞—Ü–µ–≤—Ç–∏–∫–∞ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏"""
        from playwright.async_api import async_playwright
        
        base_url = "https://ozonpharm.ru/news/press-releases/"
        news = []
        MAX_MESSAGE_LENGTH = 4000
        
        try:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –û–∑–æ–Ω –§–∞—Ä–º–∞—Ü–µ–≤—Ç–∏–∫–∞: {base_url}")
            
            async with async_playwright() as pw:
                browser = await pw.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                    viewport={'width': 1920, 'height': 1080}
                )
                page = await context.new_page()
                
                await page.goto(base_url, timeout=60000)
                await page.wait_for_selector('a.news-results__item.news-item', timeout=30000)
                
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                news_blocks = soup.select('a.news-results__item.news-item')
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(news_blocks)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –û–∑–æ–Ω –§–∞—Ä–º–∞—Ü–µ–≤—Ç–∏–∫–∞")
                
                for block in news_blocks[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    try:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                        date_block = block.select_one('div.z-date__card')
                        title_block = block.select_one('p.news-item__title span')
                        link_block = block
                        
                        if not all([date_block, title_block, link_block]):
                            logger.warning("–ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ–≤–æ—Å—Ç–Ω–æ–º –±–ª–æ–∫–µ")
                            continue
                        
                        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É
                        day = date_block.select_one('span.z-date__day').get_text(strip=True)
                        month_elements = date_block.select('span.z-date__other')
                        month = month_elements[0].get_text(strip=True) if month_elements else ''
                        year = month_elements[1].get_text(strip=True) if len(month_elements) > 1 else ''
                        date_str = f"{day} {month} {year}".strip()
                        
                        title = title_block.get_text(strip=True)
                        news_url = urljoin(base_url, link_block['href'])
                        news_id = hashlib.md5(news_url.encode()).hexdigest()
                        
                        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {date_str} | {title[:50]}...")
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                        if (await self.db.is_news_exists(news_id) or 
not any(kw in title.lower() for kw in ['–æ—Ç—á–µ—Ç', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '—Ñ–∏–Ω–∞–Ω—Å', '–¥–∏–≤–∏–¥–µ–Ω–¥'])):
                            continue
                        
                        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–æ–≤–æ—Å—Ç–∏
                        await page.goto(news_url, timeout=30000)
                        try:
                            await page.wait_for_selector('article.detail-page', timeout=20000)
                        except Exception as e:
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É: {news_url}")
                            continue
                        
                        # –ü–æ–ª—É—á–∞–µ–º –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                        news_content = await page.content()
                        news_soup = BeautifulSoup(news_content, 'html.parser')
                        content_block = news_soup.select_one('article.detail-page')
                        
                        if not content_block:
                            continue
                        
                        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã
                        financial_data = []
                        table = content_block.select_one('div.z-table__container table')
                        if table:
                            headers = [th.get_text(strip=True) for th in table.select('thead th')]
                            for row in table.select('tbody tr'):
                                cells = row.select('td')
                                if len(cells) == len(headers):
                                    metric = cells[0].get_text(strip=True)
                                    values = [cell.get_text(strip=True) for cell in cells[1:]]
                                    financial_data.append(f"{metric}: {', '.join(values)}")
                        
                        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–æ–≤
                        key_results = []
                        for li in content_block.select('li.z-list-item'):
                            text = li.get_text(' ', strip=True)
                            if any(kw in text.lower() for kw in ['–≤—ã—Ä—É—á–∫', 'ebitda', '–ø—Ä–∏–±—ã–ª', '—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç']):
                                key_results.append(f"‚Ä¢ {text}")
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                        message_lines = [
                            f"<b>#OZON #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å</b>",
                            f"<b>{title}</b> ({date_str})",
                            "",
                            "<b>–ö–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:</b>",
                            *financial_data[:8],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π
                            "",
                            "<b>–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:</b>",
                            *key_results[:5]      # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                        ]
                        
                        # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –æ–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                        message_lines = [line for line in message_lines if line.strip()]
                        message = '\n'.join(message_lines[:25])  # –ñ–µ—Å—Ç–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫
                        
                        if len(message) > MAX_MESSAGE_LENGTH:
                            message = message[:MAX_MESSAGE_LENGTH-50] + "..."
                        
                        message += f"\n\n<a href='{news_url}'>–ü–æ–¥—Ä–æ–±–Ω–µ–µ</a>"
                        
                        news.append(message)
                        await self.db.add_news(news_id, 'company_reports', title, news_url)
                        logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω –æ—Ç—á–µ—Ç: {title[:30]}...")
                        
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}", exc_info=True)
                        continue
                        
                await browser.close()
                
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}", exc_info=True)
        
        logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ. –ù–∞–π–¥–µ–Ω–æ {len(news)} –Ω–æ–≤—ã—Ö –æ—Ç—á–µ—Ç–æ–≤")
        return news
        
    async def parse_seligdar(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç—á–µ—Ç–æ–≤ –°–µ–ª–∏–≥–¥–∞—Ä —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Playwright"""
        from playwright.async_api import async_playwright
        
        base_url = "https://seligdar.ru/media/news/"
        news = []
        MAX_MESSAGE_LENGTH = 4000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
        
        try:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –°–µ–ª–∏–≥–¥–∞—Ä: {base_url}")
            
            async with async_playwright() as pw:
                browser = await pw.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                    viewport={'width': 1920, 'height': 1080}
                )
                page = await context.new_page()
                
                await page.goto(base_url, timeout=60000)
                await page.wait_for_selector('ul.list-dates', timeout=15000)
                
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                news_blocks = soup.select('ul.list-dates > li > a')
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(news_blocks)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –°–µ–ª–∏–≥–¥–∞—Ä")
                
                for block in news_blocks[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    try:
                        date_block = block.select_one('span.date')
                        title = block.get_text(strip=True).replace(date_block.get_text(strip=True), '').strip()
                        
                        if not all([date_block, title]):
                            logger.warning("–ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ–≤–æ—Å—Ç–Ω–æ–º –±–ª–æ–∫–µ –°–µ–ª–∏–≥–¥–∞—Ä")
                            continue
                        
                        date_str = date_block.get_text(strip=True)
                        news_url = urljoin(base_url, block['href'])
                        news_id = hashlib.md5(news_url.encode()).hexdigest()
                        
                        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ –°–µ–ª–∏–≥–¥–∞—Ä: {date_str} | {title[:50]}...")
                        
                        if await self.db.is_news_exists(news_id):
                            logger.debug("–ù–æ–≤–æ—Å—Ç—å –°–µ–ª–∏–≥–¥–∞—Ä —É–∂–µ –≤ –±–∞–∑–µ")
                            continue
                        
                        finance_keywords = [
                            '–æ—Ç—á–µ—Ç', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–ø—Ä–∏–±—ã–ª—å', '–≤—ã—Ä—É—á–∫', 'EBITDA', 
                            '–¥–∏–≤–∏–¥–µ–Ω–¥', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤', '–∫–≤–∞—Ä—Ç–∞–ª', '–≥–æ–¥', '–ú–°–§–û',
                            '–∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω', '—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å', 'SELG',
                            '–æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ', '–ø—Ä–æ–¥–∞–∂', '–¥–æ–±—ã—á'
                        ]
                        if not any(kw.lower() in title.lower() for kw in finance_keywords):
                            logger.debug(f"–ù–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á–µ—Ç –°–µ–ª–∏–≥–¥–∞—Ä: {title}")
                            continue
                        
                        await page.goto(news_url, timeout=30000)
                        try:
                            await page.wait_for_selector('div.block_text', timeout=10000)
                        except Exception as e:
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–æ–≤–æ—Å—Ç–∏: {news_url}")
                            continue
                        
                        news_content = await page.content()
                        news_soup = BeautifulSoup(news_content, 'html.parser')
                        
                        content_block = news_soup.select_one('div.block_text')
                        if not content_block:
                            logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–æ–≤–æ—Å—Ç–∏: {news_url}")
                            continue
                        
                        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
                        content_elements = []
                        paragraphs = content_block.find_all(['p', 'h2', 'h3', 'ul', 'table'])
                        
                        for p in paragraphs:
                            if p.name in ['p', 'h2', 'h3']:
                                text = p.get_text(strip=True)
                                if not text or len(text) < 20:
                                    continue
                                    
                                priority = 4  # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                                if any(word in text.lower() for word in ['—Ä—É–±', '$', '–º–ª—Ä–¥', '–º–ª–Ω', '%', 'EBITDA']):
                                    priority = 1  # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                                elif p.name in ['h2', 'h3']:
                                    priority = 2  # –ó–∞–≥–æ–ª–æ–≤–∫–∏
                                elif '–¥–∏–≤–∏–∑–∏–æ–Ω' in text.lower():
                                    priority = 3  # –ù–∞–∑–≤–∞–Ω–∏—è –¥–∏–≤–∏–∑–∏–æ–Ω–æ–≤
                                    
                                content_elements.append({
                                    'text': text,
                                    'tag': p.name,
                                    'priority': priority
                                })
                            
                            elif p.name == 'ul':
                                for li in p.find_all('li'):
                                    item_text = li.get_text(strip=True)
                                    if not item_text:
                                        continue
                                        
                                    priority = 3  # –ü—É–Ω–∫—Ç—ã —Å–ø–∏—Å–∫–∞
                                    if any(word in item_text.lower() for word in ['—Ä—É–±', '$', '–º–ª—Ä–¥', '–º–ª–Ω', '%']):
                                        priority = 1
                                        
                                    content_elements.append({
                                        'text': item_text,
                                        'tag': 'li',
                                        'priority': priority
                                    })
                            
                            elif p.name == 'table':
                                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã —Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º–∏
                                rows = p.find_all('tr')
                                if len(rows) > 1:
                                    headers = [th.get_text(strip=True) for th in rows[0].find_all('th')]
                                    for row in rows[1:]:
                                        cells = row.find_all('td')
                                        if len(cells) == len(headers):
                                            row_text = ' | '.join([f"{headers[i]}: {cell.get_text(strip=True)}" 
                                                                for i, cell in enumerate(cells)])
                                            content_elements.append({
                                                'text': row_text,
                                                'tag': 'table_row',
                                                'priority': 1  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                                            })
                        
                        if not content_elements:
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –°–µ–ª–∏–≥–¥–∞—Ä: {news_url}")
                            continue
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
                        message_parts = [
                            f"<b>#SELG #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å</b>",
                            f"<b>{title}</b> ({date_str})"
                        ]
                        current_length = sum(len(part) for part in message_parts)
                        
                        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (—Å–Ω–∞—á–∞–ª–∞ –≤–∞–∂–Ω—ã–µ)
                        content_elements.sort(key=lambda x: x['priority'])
                        
                        for element in content_elements:
                            new_part = ""
                            if element['tag'] in ['h2', 'h3']:
                                new_part = f"<b>{element['text']}</b>"
                            elif element['tag'] == 'table_row':
                                new_part = f"‚Ä¢ {element['text']}"
                            else:
                                # –î–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ —Å–ø–∏—Å–∫–æ–≤ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                                text = element['text']
                                if len(text) > 200:
                                    sentences = re.split(r'(?<=[.!?])\s+', text)
                                    if len(sentences) > 1:
                                        text = ' '.join(sentences[:2]) + '...'
                                    else:
                                        text = text[:200] + '...'
                                new_part = f"‚Ä¢ {text}" if element['tag'] == 'li' else text
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–º –ª–∏ –ª–∏–º–∏—Ç
                            if current_length + len(new_part) + 10 < MAX_MESSAGE_LENGTH:
                                message_parts.append(new_part)
                                current_length += len(new_part)
                            else:
                                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                                if element['priority'] <= 2 and current_length + 100 < MAX_MESSAGE_LENGTH:
                                    short_part = f"‚Ä¢ {element['text'][:150]}..." if len(element['text']) > 150 else f"‚Ä¢ {element['text']}"
                                    message_parts.append(short_part)
                                    current_length += len(short_part)
                                break
                        
                        message_parts.append(f"<a href='{news_url}'>‚Äî –°–µ–ª–∏–≥–¥–∞—Ä</a>")
                        
                        news_item = '\n'.join(message_parts)
                        news.append(news_item)
                        await self.db.add_news(news_id, 'company_reports', title, news_url)
                        logger.info(f"–ù–æ–≤–æ—Å—Ç—å –°–µ–ª–∏–≥–¥–∞—Ä —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞: {title}")
                        
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏ –°–µ–ª–∏–≥–¥–∞—Ä: {str(e)}", exc_info=True)
                        continue
                        
                await browser.close()
                
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –°–µ–ª–∏–≥–¥–∞—Ä: {str(e)}", exc_info=True)
        
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –°–µ–ª–∏–≥–¥–∞—Ä –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(news)} –Ω–æ–≤—ã—Ö –æ—Ç—á–µ—Ç–æ–≤")
        return news
        
    async def parse_pt(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç—á–µ—Ç–æ–≤ Positive Technologies —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Playwright"""
        from playwright.async_api import async_playwright
        
        base_url = "https://group.ptsecurity.com/ru/news/"
        news = []
        MAX_MESSAGE_LENGTH = 4000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
        
        try:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ Positive Technologies: {base_url}")
            
            async with async_playwright() as pw:
                browser = await pw.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                    viewport={'width': 1920, 'height': 1080}
                )
                page = await context.new_page()
                
                await page.goto(base_url, timeout=60000)
                await page.wait_for_selector('div.grid-cols-5', timeout=15000)
                await page.wait_for_timeout(2000)
                
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                news_blocks = soup.select('div.grid-cols-5 > div.col-span-3 > a.listing-item')
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(news_blocks)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤ Positive Technologies")
                
                for block in news_blocks[:10]:
                    try:
                        date_block = block.select_one('div.listing-item__date')
                        title_block = block.select_one('h2.listing-item__title')
                        
                        if not all([date_block, title_block]):
                            logger.warning("–ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ–≤–æ—Å—Ç–Ω–æ–º –±–ª–æ–∫–µ Positive Technologies")
                            continue
                        
                        date_str = date_block.get_text(strip=True)
                        title = title_block.get_text(strip=True)
                        news_url = urljoin(base_url, block['href'])
                        news_id = hashlib.md5(news_url.encode()).hexdigest()
                        
                        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ Positive Technologies: {date_str} | {title[:50]}...")
                        
                        if await self.db.is_news_exists(news_id):
                            logger.debug("–ù–æ–≤–æ—Å—Ç—å Positive Technologies —É–∂–µ –≤ –±–∞–∑–µ")
                            continue
                        
                        finance_keywords = [
                            '–æ—Ç—á–µ—Ç', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–ø—Ä–∏–±—ã–ª—å', '–≤—ã—Ä—É—á–∫', 'EBITDA', 
                            '–¥–∏–≤–∏–¥–µ–Ω–¥', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤', '–∫–≤–∞—Ä—Ç–∞–ª', '–≥–æ–¥', '–ú–°–§–û',
                            '–∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω', '—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å', 'POSI'
                        ]
                        if not any(kw.lower() in title.lower() for kw in finance_keywords):
                            logger.debug(f"–ù–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á–µ—Ç Positive Technologies: {title}")
                            continue
                        
                        await page.goto(news_url, timeout=30000)
                        try:
                            await page.wait_for_selector('article', timeout=10000)
                        except Exception as e:
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–æ–≤–æ—Å—Ç–∏: {news_url}")
                            continue
                        
                        news_content = await page.content()
                        news_soup = BeautifulSoup(news_content, 'html.parser')
                        
                        content_block = news_soup.select_one('article')
                        if not content_block:
                            logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–æ–≤–æ—Å—Ç–∏: {news_url}")
                            continue
                        
                        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
                        content_elements = []
                        paragraphs = content_block.find_all(['p', 'h2', 'h3', 'blockquote', 'div.links-block'])
                        
                        for p in paragraphs:
                            if '–ö–æ–Ω—Ç–∞–∫—Ç—ã –¥–ª—è' in p.get_text():
                                continue
                                
                            text = p.get_text(strip=True)
                            if not text or len(text) < 30 or text.startswith(('<', '[')):
                                continue
                                
                            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã: 1 - —Ü–∏—Ñ—Ä—ã/—Ñ–∏–Ω–∞–Ω—Å—ã, 2 - –∑–∞–≥–æ–ª–æ–≤–∫–∏, 3 - —Ü–∏—Ç–∞—Ç—ã, 4 - –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                            priority = 4
                            if any(word in text.lower() for word in ['—Ä—É–±', '$', '–º–ª—Ä–¥', '–º–ª–Ω', '%', 'EBITDA']):
                                priority = 1
                            elif p.name in ['h2', 'h3']:
                                priority = 2
                            elif p.name == 'blockquote':
                                priority = 3
                                
                            content_elements.append({
                                'text': text,
                                'tag': p.name,
                                'priority': priority
                            })
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ
                        links_block = content_block.select_one('div.links-block')
                        if links_block:
                            for link in links_block.find_all('a'):
                                content_elements.append({
                                    'text': link.get_text(strip=True),
                                    'tag': 'a',
                                    'priority': 1,  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Å—Å—ã–ª–æ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                                    'href': link['href']
                                })
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
                        message_parts = [
                            f"<b>#POSI #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å</b>",
                            f"<b>{title}</b> ({date_str})"
                        ]
                        current_length = sum(len(part) for part in message_parts)
                        
                        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (—Å–Ω–∞—á–∞–ª–∞ –≤–∞–∂–Ω—ã–µ)
                        content_elements.sort(key=lambda x: x['priority'])
                        
                        for element in content_elements:
                            new_part = ""
                            if element['tag'] in ['h2', 'h3']:
                                new_part = f"<b>{element['text']}</b>"
                            elif element['tag'] == 'blockquote':
                                new_part = f"üìå {element['text']}"
                            elif element['tag'] == 'a':
                                new_part = f"üìÑ <a href='{element['href']}'>{element['text']}</a>"
                            else:
                                # –î–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                                sentences = re.split(r'(?<=[.!?])\s+', element['text'])
                                if len(sentences) > 2:
                                    new_part = f"‚Ä¢ {' '.join(sentences[:2])}..."
                                else:
                                    new_part = f"‚Ä¢ {element['text']}"
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–º –ª–∏ –ª–∏–º–∏—Ç
                            if current_length + len(new_part) + 10 < MAX_MESSAGE_LENGTH:  # +10 –¥–ª—è –∑–∞–ø–∞—Å–∞
                                message_parts.append(new_part)
                                current_length += len(new_part)
                            else:
                                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                                if element['priority'] == 1 and current_length + 50 < MAX_MESSAGE_LENGTH:
                                    short_part = f"‚Ä¢ {element['text'][:100]}..." if len(element['text']) > 100 else f"‚Ä¢ {element['text']}"
                                    message_parts.append(short_part)
                                    current_length += len(short_part)
                                break
                        
                        message_parts.append(f"<a href='{news_url}'>‚Äî Positive Technologies</a>")
                        
                        news_item = '\n'.join(message_parts)
                        news.append(news_item)
                        await self.db.add_news(news_id, 'company_reports', title, news_url)
                        logger.info(f"–ù–æ–≤–æ—Å—Ç—å Positive Technologies —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞: {title}")
                        
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏ Positive Technologies: {str(e)}", exc_info=True)
                        continue
                        
                await browser.close()
                
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Positive Technologies: {str(e)}", exc_info=True)
        
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ Positive Technologies –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(news)} –Ω–æ–≤—ã—Ö –æ—Ç—á–µ—Ç–æ–≤")
        return news
        
    async def parse_sollers(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç—á–µ—Ç–æ–≤ –°–û–õ–õ–ï–†–° —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Playwright"""
        from playwright.async_api import async_playwright
        
        base_url = "https://sollers-auto.com/press-center/news/"
        news = []
        
        try:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –°–û–õ–õ–ï–†–°: {base_url}")
            
            async with async_playwright() as pw:
                browser = await pw.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                )
                page = await context.new_page()
                
                await page.goto(base_url, timeout=60000)
                await page.wait_for_selector('div.news__item', timeout=15000)
                
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                news_blocks = soup.select('div.news__item')
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(news_blocks)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –°–û–õ–õ–ï–†–°")
                
                for block in news_blocks[:10]:
                    try:
                        date_block = block.select_one('p.news-item__date')
                        title_block = block.select_one('a.news-item__title')
                        preview_block = block.select_one('p.news-item__prevText')
                        
                        if not all([date_block, title_block, preview_block]):
                            logger.warning("–ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ–≤–æ—Å—Ç–Ω–æ–º –±–ª–æ–∫–µ –°–û–õ–õ–ï–†–°")
                            continue
                        
                        date_str = date_block.get_text(strip=True)
                        title = title_block.get_text(strip=True)
                        news_url = urljoin(base_url, title_block['href'])
                        news_id = hashlib.md5(news_url.encode()).hexdigest()
                        
                        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ –°–û–õ–õ–ï–†–°: {date_str} | {title[:50]}...")
                        
                        if await self.db.is_news_exists(news_id):
                            logger.debug("–ù–æ–≤–æ—Å—Ç—å –°–û–õ–õ–ï–†–° —É–∂–µ –≤ –±–∞–∑–µ")
                            continue
                        
                        finance_keywords = [
                            '–æ—Ç—á–µ—Ç', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–ø—Ä–∏–±—ã–ª—å', '–≤—ã—Ä—É—á–∫', 'EBITDA', 
                            '–¥–∏–≤–∏–¥–µ–Ω–¥', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤', '–≥–æ–¥', '–ú–°–§–û', '–∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω',
                            '—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å', 'SVAV'
                        ]
                        if not any(kw.lower() in title.lower() for kw in finance_keywords):
                            logger.debug(f"–ù–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á–µ—Ç –°–û–õ–õ–ï–†–°: {title}")
                            continue
                        
                        await page.goto(news_url, timeout=30000)
                        await page.wait_for_selector('div.news-content__wrapper', timeout=10000)
                        
                        news_content = await page.content()
                        news_soup = BeautifulSoup(news_content, 'html.parser')
                        
                        content_block = news_soup.select_one('div.news-content__wrapper')
                        report_items = []
                        
                        if content_block:
                            paragraphs = content_block.find_all(['p', 'b'])
                            for p in paragraphs:
                                text = p.get_text(strip=True)
                                if len(text) > 30 and not text.startswith(('<', '[')):
                                    report_items.append(f"‚Ä¢ {text}")
                            
                            table = content_block.find('table')
                            if table:
                                unit = table.find('thead').find('th').get_text(strip=True)
                                rows = table.find_all('tr')[1:]
                                for row in rows:
                                    cells = row.find_all('td')
                                    if len(cells) == 2:
                                        key = cells[0].get_text(strip=True)
                                        value = cells[1].get_text(strip=True)
                                        formatted_value = f"{value} {unit}" if '%' not in value else value
                                        report_items.append(f"  - {key}: {formatted_value}")
                        
                        if not report_items:
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –°–û–õ–õ–ï–†–°: {news_url}")
                            continue
                        
                        message_lines = [
                            f"<b>#SVAV #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å</b>",
                            f"<b>{title}</b> ({date_str})",
                            *report_items[:8],
                            f"<a href='{news_url}'>‚Äî –°–û–õ–õ–ï–†–°</a>"
                        ]
                        
                        news_item = '\n'.join(message_lines)
                        news.append(news_item)
                        await self.db.add_news(news_id, 'company_reports', title, news_url)
                        logger.info(f"–ù–æ–≤–æ—Å—Ç—å –°–û–õ–õ–ï–†–° —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞: {title}")
                        
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏ –°–û–õ–õ–ï–†–°: {str(e)}", exc_info=True)
                        continue
                        
                await browser.close()
                
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –°–û–õ–õ–ï–†–°: {str(e)}", exc_info=True)
        
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –°–û–õ–õ–ï–†–° –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(news)} –Ω–æ–≤—ã—Ö –æ—Ç—á–µ—Ç–æ–≤")
        return news
        
    async def parse_vk(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç—á–µ—Ç–æ–≤ VK —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Playwright"""
        from playwright.async_api import async_playwright
        
        base_url = "https://vk.company/ru/press/releases/"
        news = []
        
        try:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ VK: {base_url}")
            
            async with async_playwright() as pw:
                browser = await pw.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                )
                page = await context.new_page()
                
                await page.goto(base_url, timeout=60000)
                await page.wait_for_selector('div.Publications_publicationItem__ICFNd', timeout=15000)
                
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                news_blocks = soup.select('div.Publications_publicationItem__ICFNd')
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(news_blocks)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤ VK")
                
                for block in news_blocks[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    try:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                        date_block = block.select_one('div.Publications_publicationSubtitle__e297T')
                        title_block = block.select_one('div.Publications_publicationTitle__oKOtT')
                        link_block = block.select_one('a.Publications_publication__Ehhcu')
                        
                        if not all([date_block, title_block, link_block]):
                            logger.warning("–ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ–≤–æ—Å—Ç–Ω–æ–º –±–ª–æ–∫–µ VK")
                            continue
                        
                        date_str = date_block.get_text(strip=True)
                        title = title_block.get_text(strip=True)
                        news_url = urljoin(base_url, link_block['href'])
                        news_id = hashlib.md5(news_url.encode()).hexdigest()
                        
                        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ VK: {date_str} | {title[:50]}...")
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                        if await self.db.is_news_exists(news_id):
                            logger.debug("–ù–æ–≤–æ—Å—Ç—å VK —É–∂–µ –≤ –±–∞–∑–µ")
                            continue
                        
                        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                        finance_keywords = [
                            '–æ—Ç—á–µ—Ç', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–ø—Ä–∏–±—ã–ª—å', '–≤—ã—Ä—É—á–∫', 'EBITDA', 
                            '–¥–∏–≤–∏–¥–µ–Ω–¥', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤', '–∫–≤–∞—Ä—Ç–∞–ª', '–≥–æ–¥', '–º–µ—Å—è—Ü',
                            '–æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ', '—Ä–µ–ª–∏–∑', '–∞–Ω–∞–ª–∏–∑'
                        ]
                        if not any(kw.lower() in title.lower() for kw in finance_keywords):
                            logger.debug(f"–ù–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á–µ—Ç VK: {title}")
                            continue
                        
                        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–æ–≤–æ—Å—Ç–∏
                        await page.goto(news_url, timeout=30000)
                        await page.wait_for_selector('div.publication-content', timeout=10000)
                        
                        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–æ–≤–æ—Å—Ç–∏
                        news_content = await page.content()
                        news_soup = BeautifulSoup(news_content, 'html.parser')
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—É–Ω–∫—Ç—ã –æ—Ç—á–µ—Ç–∞
                        content_block = news_soup.select_one('div.publication-content')
                        report_items = []
                        
                        if content_block:
                            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–∞–∂–Ω—ã–µ –ø—É–Ω–∫—Ç—ã (–∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Å–ø–∏—Å–∫–∏)
                            strong_headers = content_block.find_all(['strong', 'p'])
                            ul_blocks = content_block.find_all('ul')
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                            for header in strong_headers:
                                text = header.get_text(strip=True)
                                if len(text) > 30 and not text.startswith(('[')):  # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —Å–Ω–æ—Å–∫–∏
                                    report_items.append(f"‚Ä¢ {text}")
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º –ø—É–Ω–∫—Ç—ã —Å–ø–∏—Å–∫–æ–≤
                            for ul in ul_blocks:
                                for li in ul.find_all('li'):
                                    item_text = li.get_text(strip=True)
                                    if item_text:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –ø—É–Ω–∫—Ç—ã
                                        report_items.append(f"  - {item_text}")
                        
                        if not report_items:
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ VK: {news_url}")
                            continue
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                        message_lines = [
                            f"<b>#VKCO #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å</b>",
                            f"<b>{title}</b> ({date_str})",
                            *report_items[:8],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—É–Ω–∫—Ç–æ–≤
                            f"<a href='{news_url}'>‚Äî VK</a>"
                        ]
                        
                        news_item = '\n'.join(message_lines)
                        news.append(news_item)
                        await self.db.add_news(news_id, 'company_reports', title, news_url)
                        logger.info(f"–ù–æ–≤–æ—Å—Ç—å VK —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞: {title}")
                        
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏ VK: {str(e)}", exc_info=True)
                        continue
                        
                await browser.close()
                
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ VK: {str(e)}", exc_info=True)
        
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ VK –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(news)} –Ω–æ–≤—ã—Ö –æ—Ç—á–µ—Ç–æ–≤")
        return news
        
    async def parse_mmk(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç—á–µ—Ç–æ–≤ –ú–ú–ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Playwright"""
        from playwright.async_api import async_playwright
        
        base_url = "https://mmk.ru/ru/press-center/news/"
        news = []
        
        try:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –ú–ú–ö: {base_url}")
            
            async with async_playwright() as pw:
                browser = await pw.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                )
                page = await context.new_page()
                
                await page.goto(base_url, timeout=60000)
                await page.wait_for_selector('div.card-news-list__card', timeout=15000)
                
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                news_blocks = soup.select('div.card-news-list__card')
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(news_blocks)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –ú–ú–ö")
                
                for block in news_blocks[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    try:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                        date_block = block.select_one('span.card-article__date')
                        title_block = block.select_one('div.card-article__title')
                        link_block = block.select_one('a.card-article__link')
                        
                        if not all([date_block, title_block, link_block]):
                            logger.warning("–ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ–≤–æ—Å—Ç–Ω–æ–º –±–ª–æ–∫–µ –ú–ú–ö")
                            continue
                        
                        date_str = date_block.get_text(strip=True)
                        title = title_block.get_text(strip=True)
                        news_url = urljoin(base_url, link_block['href'])
                        news_id = hashlib.md5(news_url.encode()).hexdigest()
                        
                        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ –ú–ú–ö: {date_str} | {title[:50]}...")
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                        if await self.db.is_news_exists(news_id):
                            logger.debug("–ù–æ–≤–æ—Å—Ç—å –ú–ú–ö —É–∂–µ –≤ –±–∞–∑–µ")
                            continue
                        
                        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                        finance_keywords = [
                            '–æ—Ç—á–µ—Ç', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–ø—Ä–∏–±—ã–ª—å', '–≤—ã—Ä—É—á–∫', 'EBITDA', 
                            '–¥–∏–≤–∏–¥–µ–Ω–¥', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤', '–∫–≤–∞—Ä—Ç–∞–ª', '–≥–æ–¥', '–º–µ—Å—è—Ü'
                        ]
                        if not any(kw.lower() in title.lower() for kw in finance_keywords):
                            logger.debug(f"–ù–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á–µ—Ç –ú–ú–ö: {title}")
                            continue
                        
                        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–æ–≤–æ—Å—Ç–∏
                        await page.goto(news_url, timeout=30000)
                        await page.wait_for_selector('div.text-editor__content', timeout=10000)
                        
                        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–æ–≤–æ—Å—Ç–∏
                        news_content = await page.content()
                        news_soup = BeautifulSoup(news_content, 'html.parser')
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—É–Ω–∫—Ç—ã –æ—Ç—á–µ—Ç–∞
                        content_block = news_soup.select_one('div.text-editor__content')
                        report_items = []
                        
                        if content_block:
                            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—É–Ω–∫—Ç—ã —Å–ø–∏—Å–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                            ul_blocks = content_block.find_all('ul')
                            for ul in ul_blocks:
                                for li in ul.find_all('li'):
                                    item_text = li.get_text(strip=True)
                                    report_items.append(f"‚Ä¢ {item_text}")
                            
                            # –ï—Å–ª–∏ –Ω–µ—Ç —Å–ø–∏—Å–∫–∞, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –∞–±–∑–∞—Ü–∞ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                            if not report_items:
                                paragraphs = content_block.find_all('p')
                                for p in paragraphs[:3]:
                                    text = p.get_text(strip=True)
                                    if text and len(text) > 20:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã
                                        report_items.append(text)
                        
                        if not report_items:
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –ú–ú–ö: {news_url}")
                            continue
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                        message_lines = [
                            f"<b>#MAGN #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å</b>",
                            f"<b>{title}</b> ({date_str})",
                            *report_items[:5],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—É–Ω–∫—Ç–æ–≤
                            f"<a href='{news_url}'>‚Äî –ú–ú–ö</a>"
                        ]
                        
                        news_item = '\n'.join(message_lines)
                        news.append(news_item)
                        await self.db.add_news(news_id, 'company_reports', title, news_url)
                        logger.info(f"–ù–æ–≤–æ—Å—Ç—å –ú–ú–ö —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞: {title}")
                        
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏ –ú–ú–ö: {str(e)}", exc_info=True)
                        continue
                        
                await browser.close()
                
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ú–ú–ö: {str(e)}", exc_info=True)
        
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –ú–ú–ö –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(news)} –Ω–æ–≤—ã—Ö –æ—Ç—á–µ—Ç–æ–≤")
        return news

    async def parse_inarctica(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç—á–µ—Ç–æ–≤ –ò–Ω–∞—Ä–∫—Ç–∏–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Playwright"""
        from playwright.async_api import async_playwright
        
        base_url = "https://inarctica.com/media/news/"
        news = []
        
        try:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –ò–Ω–∞—Ä–∫—Ç–∏–∫–∏: {base_url}")
            
            async with async_playwright() as pw:
                # –ó–∞–ø—É—Å–∫–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –≤ headless —Ä–µ–∂–∏–º–µ
                browser = await pw.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                )
                page = await context.new_page()
                
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∂–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                await page.goto(base_url, timeout=60000)
                await page.wait_for_selector('article.news-block', timeout=15000)
                
                # –ü–æ–ª—É—á–∞–µ–º HTML –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –±–ª–æ–∫–∏
                news_blocks = soup.select('article.news-block')
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(news_blocks)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤")
                
                for block in news_blocks[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    try:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                        date_block = block.select_one('div.news-block__date')
                        title_block = block.select_one('h3.h3')
                        link_block = block.select_one('a.btn-accent-link')
                        
                        if not all([date_block, title_block, link_block]):
                            logger.warning("–ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–æ–≤–æ—Å—Ç–Ω–æ–º –±–ª–æ–∫–µ")
                            continue
                        
                        date_str = ' '.join(date_block.stripped_strings)
                        title = title_block.get_text(strip=True)
                        news_url = urljoin(base_url, link_block['href'])
                        news_id = hashlib.md5(news_url.encode()).hexdigest()
                        
                        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏: {date_str} | {title[:50]}...")
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                        if await self.db.is_news_exists(news_id):
                            logger.debug("–ù–æ–≤–æ—Å—Ç—å —É–∂–µ –≤ –±–∞–∑–µ")
                            continue
                        
                        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –Ω–æ–≤–æ—Å—Ç–∏
                        finance_keywords = ['–æ—Ç—á–µ—Ç', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–ø—Ä–∏–±—ã–ª—å', '–≤—ã—Ä—É—á–∫', 'EBITDA', '–¥–∏–≤–∏–¥–µ–Ω–¥']
                        if not any(kw.lower() in title.lower() for kw in finance_keywords):
                            logger.debug("–ù–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç—á–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                            continue
                        
                        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–æ–≤–æ—Å—Ç–∏
                        await page.goto(news_url, timeout=30000)
                        await page.wait_for_selector('div.article__content', timeout=10000)
                        
                        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–æ–≤–æ—Å—Ç–∏
                        news_content = await page.content()
                        news_soup = BeautifulSoup(news_content, 'html.parser')
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—É–Ω–∫—Ç—ã –æ—Ç—á–µ—Ç–∞
                        content_block = news_soup.select_one('div.article__content')
                        report_items = []
                        
                        if content_block:
                            ul_block = content_block.find('ul')
                            if ul_block:
                                for li in ul_block.find_all('li'):
                                    item_text = li.get_text(strip=True)
                                    report_items.append(f"‚Ä¢ {item_text}")
                            else:
                                first_p = content_block.find('p')
                                if first_p:
                                    report_items.append(first_p.get_text(strip=True))
                        
                        if not report_items:
                            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞")
                            continue
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                        message_lines = [
                            f"<b>#AQUA #–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å</b>",
                            f"<b>{title}</b> ({date_str})",
                            *report_items[:5],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—É–Ω–∫—Ç–æ–≤
                            f"<a href='{news_url}'>‚Äî –ò–Ω–∞—Ä–∫—Ç–∏–∫–∞</a>"
                        ]
                        
                        news_item = '\n'.join(message_lines)
                        news.append(news_item)
                        await self.db.add_news(news_id, 'company_reports', title, news_url)
                        logger.info("–ù–æ–≤–æ—Å—Ç—å —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞")
                        
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {str(e)}", exc_info=True)
                        continue
                        
                await browser.close()
                
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}", exc_info=True)
        
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(news)} –Ω–æ–≤—ã—Ö –æ—Ç—á–µ—Ç–æ–≤")
        return news

    

    async def parse(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö –∫–æ–º–ø–∞–Ω–∏–π"""
        has_news = False
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –ò–Ω–∞—Ä–∫—Ç–∏–∫–∏
        inarctica_news = await self.parse_inarctica()
        if isinstance(inarctica_news, list) and inarctica_news:
            for news_item in inarctica_news:
                await self.tg.safe_send(f"üìä <b>–û–¢–ß–ï–¢–´ –ö–û–ú–ü–ê–ù–ò–ô</b>\n{news_item}", 
                                    parse_mode='HTML',
                                    content_type='news')
                await asyncio.sleep(4)
                has_news = True
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –ú–ú–ö
        mmk_news = await self.parse_mmk()
        if isinstance(mmk_news, list) and mmk_news:
            for news_item in mmk_news:
                await self.tg.safe_send(f"üìä <b>–û–¢–ß–ï–¢–´ –ö–û–ú–ü–ê–ù–ò–ô</b>\n{news_item}", 
                                    parse_mode='HTML',
                                    content_type='news')
                await asyncio.sleep(4)
                has_news = True
                
        # –ü–∞—Ä—Å–∏–Ω–≥ VK
        vk_news = await self.parse_vk()
        if isinstance(vk_news, list) and vk_news:
            for news_item in vk_news:
                await self.tg.safe_send(f"üìä <b>–û–¢–ß–ï–¢–´ –ö–û–ú–ü–ê–ù–ò–ô</b>\n{news_item}", 
                                    parse_mode='HTML',
                                    content_type='news')
                await asyncio.sleep(4)
                has_news = True
                
        # –ü–∞—Ä—Å–∏–Ω–≥ –°–û–õ–õ–ï–†–°
        sollers_news = await self.parse_sollers()
        if isinstance(sollers_news, list) and sollers_news:
            for news_item in sollers_news:
                await self.tg.safe_send(f"üìä <b>–û–¢–ß–ï–¢–´ –ö–û–ú–ü–ê–ù–ò–ô</b>\n{news_item}", 
                                    parse_mode='HTML',
                                    content_type='news')
                await asyncio.sleep(4)
                has_news = True

        # –ü–∞—Ä—Å–∏–Ω–≥ Positive Technologies
        pt_news = await self.parse_pt()
        if isinstance(pt_news, list) and pt_news:
            for news_item in pt_news:
                await self.tg.safe_send(f"üìä <b>–û–¢–ß–ï–¢–´ –ö–û–ú–ü–ê–ù–ò–ô</b>\n{news_item}", 
                                    parse_mode='HTML',
                                    content_type='news')
                await asyncio.sleep(4)
                has_news = True
                
        # –ü–∞—Ä—Å–∏–Ω–≥ –°–µ–ª–∏–≥–¥–∞—Ä
        seligdar_news = await self.parse_seligdar()
        if isinstance(seligdar_news, list) and seligdar_news:
            for news_item in seligdar_news:
                await self.tg.safe_send(f"üìä <b>–û–¢–ß–ï–¢–´ –ö–û–ú–ü–ê–ù–ò–ô</b>\n{news_item}", 
                                    parse_mode='HTML',
                                    content_type='news')
                await asyncio.sleep(4)
                has_news = True
                
        ozon_news = await self.parse_ozonpharm()
        if isinstance(ozon_news, list) and ozon_news:
            for news_item in ozon_news:
                await self.tg.safe_send(f"üìä <b>–û–¢–ß–ï–¢–´ –ö–û–ú–ü–ê–ù–ò–ô</b>\n{news_item}", 
                                    parse_mode='HTML',
                                    content_type='news')
                await asyncio.sleep(4)
                has_news = True
        
        if not has_news:
            await self.tg.safe_send("‚ÑπÔ∏è –ù–µ—Ç –Ω–æ–≤—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π",
                                content_type='news')

        return has_news